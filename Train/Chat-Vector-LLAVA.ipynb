{"cells":[{"cell_type":"markdown","id":"520871e0-f7b7-4c3a-a2f8-9e3d6829d556","metadata":{"id":"520871e0-f7b7-4c3a-a2f8-9e3d6829d556"},"source":["#### Chat Vector\n","- LLM의 학습된 가중치 매개변수를 더하여 사전 학습된 모델에 상호작용 능력을 부여하는 기술\n","- 한국어 사전학습 모델, 그리고 image-to-text 2개를 이용\n","- 이미지에 대한 텍스트 생성 모델에서 이미지 생성 부분과 언어 모델의 특정 레이어를 제외하고, 나머지 언어 생성과 관련한 레이어는 한국어 사전학습 모델의 가중치를 뺀 값을 추가하여, 이미지에 대한 설명을 한국어로 할 수 있도록 조정한 방식"]},{"cell_type":"code","execution_count":null,"id":"ecd4d945-5181-418b-b066-aba69b0fca59","metadata":{"id":"ecd4d945-5181-418b-b066-aba69b0fca59","outputId":"d5ad06a6-94c2-4ea5-e170-20ec54f03de3","colab":{"referenced_widgets":["2392e5000cdf44cb9ffe31e6e5243d11","2ab75bad976b481382cb2959ef474eeb"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2392e5000cdf44cb9ffe31e6e5243d11","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ab75bad976b481382cb2959ef474eeb","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoProcessor, LlavaForConditionalGeneration\n","from PIL import Image\n","import requests\n","\n","# 영어 LLM 및 멀티모달 비전 모델 로드\n","base_model_name = \"xtuner/llava-llama-3-8b-transformers\"\n","base_model = LlavaForConditionalGeneration.from_pretrained(\n","    base_model_name,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    cache_dir='/data'\n",")\n","\n","# 한국어 사전학습 모델 로드\n","ko_model_name = \"beomi/Llama-3-KoEn-8B\"\n","inst_model = AutoModelForCausalLM.from_pretrained(\n","    ko_model_name,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    cache_dir='/data'\n",")"]},{"cell_type":"code","execution_count":null,"id":"c68321c4-bbab-4936-8724-63b60fdca868","metadata":{"id":"c68321c4-bbab-4936-8724-63b60fdca868"},"outputs":[],"source":["# 결합시 제외할 레이어 지정\n","skip_layers = [\n","    \"model.embed_tokens.weight\",\n","    \"lm_head.weight\",\n","    \"vision_tower.vision_model.embeddings.class_embedding\",\n","    \"vision_tower.vision_model.embeddings.patch_embedding.weight\",\n","    \"vision_tower.vision_model.embeddings.position_embedding.weight\",\n","    \"vision_tower.vision_model.pre_layrnorm.weight\"\n","]\n","\n","# 새로운 모델 상태 사전 생성\n","new_state_dict = base_model.state_dict()\n","\n","# 가중치 업데이트\n","for k, v in inst_model.state_dict().items():\n","    if any(skip in k for skip in skip_layers) or (\"layernorm\" in k):\n","        continue\n","    if k in new_state_dict:\n","        chat_vector = base_model.state_dict()[k] - inst_model.state_dict().get(k, torch.zeros_like(v))\n","        new_v = v + chat_vector.to(v.device)\n","        base_model.copy_(new_v)"]},{"cell_type":"code","execution_count":null,"id":"774d46a9-22f5-4e6e-aace-cb9d75145332","metadata":{"id":"774d46a9-22f5-4e6e-aace-cb9d75145332","outputId":"4342bd6e-399c-4e11-e606-860924a61c3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-06-17 10:18:19,739] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]}],"source":["base_model.save_pretrained('./ko-llava')"]},{"cell_type":"code","execution_count":null,"id":"443eea4b-4b9f-404c-bc0e-144ad744878a","metadata":{"id":"443eea4b-4b9f-404c-bc0e-144ad744878a","outputId":"2c8f2e0e-c2c3-497e-bcdd-43dc4d18895e","colab":{"referenced_widgets":["995d5505771c4739acb02c90b54bfb57"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"995d5505771c4739acb02c90b54bfb57","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import requests\n","from PIL import Image\n","\n","import torch\n","from transformers import AutoProcessor, LlavaForConditionalGeneration, TextStreamer\n","\n","model_id = \"./ko-llava\"\n","\n","model = LlavaForConditionalGeneration.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map='auto',\n","    revision='a38aac3', # 'a38aac3' for basic ChatVector, '4f04d1e' for Model diff based merging(ref. https://huggingface.co/blog/maywell/llm-feature-transfer)\n",")\n","\n","processor = AutoProcessor.from_pretrained('xtuner/llava-llama-3-8b-transformers')\n","\n","tokenizer = processor.tokenizer\n","terminators = [\n","    tokenizer.eos_token_id,\n","    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","]\n","streamer = TextStreamer(tokenizer)\n","\n","prompt = (\"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\n이 이미지에 대해서 설명해주세요.<|eot_id|>\"\n","          \"<|start_header_id|>assistant<|end_header_id|>\\n\\n이 이미지에는\")\n","image_file = \"https://cdn-uploads.huggingface.co/production/uploads/5e56829137cb5b49818287ea/NWfoArWI4UPAxpEnolkwT.jpeg\"\n","\n","raw_image = Image.open(requests.get(image_file, stream=True).raw)\n","inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)"]},{"cell_type":"code","execution_count":null,"id":"5b20eaa5-6209-41e5-a8b3-b603a39ebf87","metadata":{"id":"5b20eaa5-6209-41e5-a8b3-b603a39ebf87","outputId":"f6624381-38d3-4842-8a01-db32af254955"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","<image>\n","이 이미지에 대해서 설명해주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","이 이미지에는 큰 고양이가 물 위를 걷고 있는 모습을 보여줍니다. 고양이는 전체적으로 대형이며, 특히 몸통은 매우 큰 것으로 보입니다. 고유한 필름 처리로 인해, 고양이의 모습이 더욱 선명하게 보이네요.\n","\n","고양이는 물 위에 걸려 있어, 한 발을 떨며 걷기 시작합니다. 고민하는 눈으로, 물 위의 움직임을 감시하면서 걷어나가고 있습니다. 이 고양이나 물 위 걷기를 잘하다고 할 수 있습니다.<|eot_id|>\n","user<|end_header_id|>\n","\n","<image>\n","이 이미지에 대해서 설명해주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","이 이미지에는 큰 고양이가 물 위를 걷고 있는 모습을 보여줍니다. 고양이는 전체적으로 대형이며, 특히 몸통은 매우 큰 것으로 보입니다. 고유한 필름 처리로 인해, 고양이의 모습이 더욱 선명하게 보이네요.\n","\n","고양이는 물 위에 걸려 있어, 한 발을 떨며 걷기 시작합니다. 고민하는 눈으로, 물 위의 움직임을 감시하면서 걷어나가고 있습니다. 이 고양이나 물 위 걷기를 잘하다고 할 수 있습니다.<|eot_id|>\n"]}],"source":["output = model.generate(\n","    **inputs,\n","    max_new_tokens=512,\n","    do_sample=True,  # do_sample을 True로 설정하여 샘플링을 사용합니다.\n","    eos_token_id=terminators,\n","    no_repeat_ngram_size=3,  # n-gram의 반복을 방지합니다.\n","    temperature=0.7,  # 출력을 다양화하기 위해 온도를 설정합니다.\n","    top_p=0.9,  # top-p 샘플링을 사용하여 출력을 다양화합니다.\n","    streamer=streamer\n",")\n","print(processor.decode(output[0][2:], skip_special_tokens=False))"]},{"cell_type":"code","execution_count":null,"id":"9eeb4908-0678-4ed2-9dde-443a449b100f","metadata":{"id":"9eeb4908-0678-4ed2-9dde-443a449b100f","outputId":"4d9868c0-0c13-4654-ab7c-5b568c526146","colab":{"referenced_widgets":["c0a21df7a6664fe7826bd2bbed2858b0"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0a21df7a6664fe7826bd2bbed2858b0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import requests\n","from PIL import Image\n","\n","import torch\n","from transformers import AutoProcessor, LlavaForConditionalGeneration, TextStreamer\n","\n","model_id = \"./ko-llava\"\n","\n","model = LlavaForConditionalGeneration.from_pretrained(\n","    model_id,\n","    torch_dtype='auto',\n","    device_map='auto',\n","    revision='a38aac3', # 'a38aac3' for basic ChatVector, '4f04d1e' for Model diff based merging(ref. https://huggingface.co/blog/maywell/llm-feature-transfer)\n",")\n","\n","processor = AutoProcessor.from_pretrained('xtuner/llava-llama-3-8b-transformers')\n","\n","tokenizer = processor.tokenizer\n","terminators = [\n","    tokenizer.eos_token_id,\n","    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","]\n","streamer = TextStreamer(tokenizer)\n","\n","prompt = (\"<|start_header_id|>user<|end_header_id|>\\n\\n<image>\\n대한민국의 어디를 나타내는걸까요?.<|eot_id|>\"\n","          \"<|start_header_id|>assistant<|end_header_id|>\\n\\n이 이미지에는\")\n","image_file = \"https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyNDAzMzFfNDYg%2FMDAxNzExODMzNTM4MTU5.Xf8te7rReNi4aXtFAsjjdeCsXDv1Tr4Be5pOsuofd0Mg.i8UclMMaD91i0MEMEXXKsgloQKZQbJfVJQeqK_2UC8Yg.PNG%2F359d2185%25A3%25ADc597%25A3%25AD49a3%25A3%25ADb102%25A3%25ADdf25158be59f.png&type=sc960_832\"\n","\n","raw_image = Image.open(requests.get(image_file, stream=True).raw)\n","inputs = processor(prompt, raw_image, return_tensors='pt').to(0, torch.float16)"]},{"cell_type":"code","execution_count":null,"id":"9b51bcda-4426-4f4f-8e1b-386a324d64e7","metadata":{"id":"9b51bcda-4426-4f4f-8e1b-386a324d64e7","outputId":"8e4853db-8e97-4ec8-cb48-280a6be186bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","<image>\n","대한민국의 어디를 나타내는걸까요?.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","이 이미지에는 서울 시내가 대개 묘사되어 있습니다. 서울의 많은 건물과 도시의 전망이 잘 보이는 모습으로, 서울의 대표적인 도시 풍경이 담겨 있습니다.<|eot_id|>\n","user<|end_header_id|>\n","\n","<image>\n","대한민국의 어디를 나타내는걸까요?.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","이 이미지에는 서울 시내가 대개 묘사되어 있습니다. 서울의 많은 건물과 도시의 전망이 잘 보이는 모습으로, 서울의 대표적인 도시 풍경이 담겨 있습니다.<|eot_id|>\n"]}],"source":["output = model.generate(\n","    **inputs,\n","    max_new_tokens=512,\n","    do_sample=True,  # do_sample을 True로 설정하여 샘플링을 사용합니다.\n","    eos_token_id=terminators,\n","    no_repeat_ngram_size=3,  # n-gram의 반복을 방지합니다.\n","    temperature=0.7,  # 출력을 다양화하기 위해 온도를 설정합니다.\n","    top_p=0.9,  # top-p 샘플링을 사용하여 출력을 다양화합니다.\n","    streamer=streamer\n",")\n","print(processor.decode(output[0][2:], skip_special_tokens=False))"]}],"metadata":{"kernelspec":{"display_name":"moon","language":"python","name":"moon"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}